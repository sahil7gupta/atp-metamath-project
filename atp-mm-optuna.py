# -*- coding: utf-8 -*-
"""nn_transformer_learn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLPsHV3I5TKQr1JxlDhWLkAmYPE7e9tS
"""

#import os
#from google.colab import drive
#import torch as tr
#drive.mount('/content/drive')
#DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/mmpy/'
#os.chdir(DATA_DIR)
#!ls

#device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')
#!pip install torcheval

# data preprocessing
# some of this will move to other file in data preprocessing

import optuna
from optuna.trial import TrialState
import torch.optim as optim
import random
import pickle


with open('data_new.pkl', 'rb') as file:
  # Call load method to deserialze
  data = pickle.load(file)
print(len(data))

# using only 10000 rows out of 3.7M rows
data_to_use = 10000
data = data[:data_to_use]
print(len(data))

print(random.choice(data))

vocab_in = {k for i in data for k in i[0]}
vocab_in = dict.fromkeys(vocab_in,0)
vocab_in.update((k, i) for i, k in enumerate(vocab_in,1))
#print(vocab_in)
vocab_in_size = len(vocab_in)
print(vocab_in_size)


vocab_out = {i[1] for i in data}
vocab_out = dict.fromkeys(vocab_out,0)
vocab_out.update((k, i) for i, k in enumerate(vocab_out))
#print(vocab_out)
vocab_out_size = len(vocab_out)
print(vocab_out_size)

# from collections import Counter
# max_len = 0
# max_seq = ""
# seq_len = []
# for i in data:
#   new_len = len(i[0])
#   seq_len.append(new_len)
#   if new_len > max_len:
#       max_len = max(max_len, new_len)
#       max_seq = i
# # print(max_len)
# # print(max_seq)
# c = Counter(seq_len)
# print(c)

# data_full_available = 0
# k=150
# for i in c.most_common():
#   if k>0:
#     # print(i[0])
#     data_full_available += i[1]
#     k=k-1
#   else:
#     break
# print (data_full_available)

from collections import Counter

labels_list = [row[1] for row in data]
c = Counter(labels_list)
#print(c)
#print(len(labels_list))

import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler

test_split = .2
shuffle_dataset = True
random_seed= 42

# Creating data indices for training and validation splits:
dataset_size = len(data)
indices = list(range(dataset_size))
split = int(np.floor(test_split * dataset_size))
if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, test_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
test_sampler = SubsetRandomSampler(test_indices)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

# torch.manual_seed(12345)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

vocab_size = vocab_in_size + 1

block_size = 200 # context length

num_classes = vocab_out_size
drop_prob = 0.13

# batch_size = 2048
epochs = 10
# epochs = 1

model_path = 'model_classification.pth'
model_loader = False



class DataSet(Dataset):
	def __init__(self, data):
		super().__init__()
		# self.imdb_data = torch.tensor(torch.load(f"imdb/imdb_{mode}.json"))
		self.data = data

	def __getitem__(self, idx):
		item = self.data[idx]
		# data = self.imdb_data[idx]
		# The last element is the target
		seq = item[0][1:201]
		seq = torch.Tensor([vocab_in[x] for x in seq]).type(torch.LongTensor)
		# print(seq.shape)
		seq = F.pad(seq, pad=(0,  200 - seq.shape[0]), value = 0)
		mask = (seq == 0).to(device)
		targets = torch.zeros(num_classes, device=device)
		targets[vocab_out[item[1]]] = 1
		return seq, targets, mask

	def __len__(self):
		return len(self.data)


complete_dataset = DataSet(data)

def getData(batch_size):

	train_data = DataLoader(complete_dataset, batch_size, sampler=train_sampler)
	test_data = DataLoader(complete_dataset, batch_size, sampler=test_sampler)
	
	return train_data, test_data

# print(dataset_X[0])
# (input,target) = train_data[0]
# print(input, target)

import math
from torcheval.metrics import MulticlassAccuracy
import copy

# class Norm(nn.Module):
# 	def __init__(self, d_model, eps = 1e-6):
# 		super().__init__()

# 		self.size = d_model
# 		# create two learnable parameters to calibrate normalisation
# 		self.alpha = nn.Parameter(torch.ones(self.size))
# 		self.bias = nn.Parameter(torch.zeros(self.size))
# 		self.eps = eps
# 	def forward(self, x):
# 		norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
# 		/ (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
# 		return norm

class block(nn.Module):
	def __init__(self, embeds_size, num_heads):
		super(block, self).__init__()
		self.attention = nn.MultiheadAttention(embeds_size, num_heads, batch_first=True)
		self.ffn = nn.Sequential(
			nn.Linear(embeds_size, 2 * embeds_size),
			nn.LeakyReLU(),
			nn.Linear(2 * embeds_size, embeds_size),
		)
		# self.drop1 = nn.Dropout(drop_prob)
		# self.drop2 = nn.Dropout(drop_prob)
		self.ln1 = nn.LayerNorm(embeds_size)
		self.ln2 = nn.LayerNorm(embeds_size)
		# self.norm_1 = Norm(embeds_size)

	def forward(self, hidden_state, pad_mask=None):
		attn, _ = self.attention(hidden_state, hidden_state, hidden_state, need_weights=False, key_padding_mask = pad_mask)
		# attn = self.drop1(attn)
		out = self.ln1(hidden_state + attn)
		observed = self.ffn(out)
		# observed = self.drop2(observed)
		out = self.ln2(out + observed)
		# out = self.norm_1(out)
		return out

class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

def get_clones(module, N):
	return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class transformer(nn.Module):
	def __init__(self, embeds_size, num_heads, n_enc_blocks):
		super(transformer, self).__init__()
		
		self.embeds_size = embeds_size

		self.tok_emb = nn.Embedding(vocab_size, embeds_size)
		# self.pos_emb = nn.Embedding(block_size, embeds_size)
		self.pos_emb = PositionalEncoding(embeds_size)

		self.n_enc_blocks = n_enc_blocks
		self.block_layers = get_clones(block(embeds_size, num_heads), self.n_enc_blocks)

		# self.block = block()

		# self.block1 = block()


		# self.block2 = block()

		# self.ln1 = nn.LayerNorm(embeds_size)
		# self.ln2 = nn.LayerNorm(embeds_size)

		self.classifier_head = nn.Sequential(
			# nn.Linear(embeds_size, embeds_size//10),
		# 	nn.LeakyReLU(),
		# 	# nn.Dropout(drop_prob),
		# 	# nn.Linear(embeds_size, embeds_size),
			# nn.LeakyReLU(),
			nn.Linear(embeds_size, num_classes),
		# 	# nn.Softmax(dim=1),
		)

		# self.classifier_head = nn.Sequential(
		# 	nn.Linear(embeds_size, embeds_size),
		# 	nn.LeakyReLU(),
		# 	nn.Linear(10 * embeds_size, num_classes),
		# )


		print("number of parameters: %.2fM" % (self.num_params()/1e6,))

	def num_params(self):
		n_params = sum(p.numel() for p in self.parameters())
		return n_params


	def forward(self, seq, pad_mask=None):
		B,T = seq.shape
		embedded = self.tok_emb(seq) * math.sqrt(self.embeds_size)
		# embedded = embedded + self.pos_emb(torch.arange(T, device=device))
		output = self.pos_emb(embedded)

		# mask = [pad_mask, None]
		for i in range(self.n_enc_blocks):
			output = self.block_layers[i](output, pad_mask)

		# Block 1
		# output = self.block(output, pad_mask)
		# output = self.block1(output, pad_mask)
		# output = self.block2(output, pad_mask)
		# print(output)
		# print("output before mean")
		# print(output.shape)
		# output = output.mean(dim=1)
		# print("output after mean")
		# print(output.shape)

		# Block 2
		# output = self.block2(output)
		# print(output)
		# print(output.shape)
		output = output.mean(dim=1)
		# print(output.shape)
		output = self.classifier_head(output)
		return output




# if model_loader:
	# model.load_state_dict(torch.load(model_path))
# model.to(device)
# model_loss = nn.BCEWithLogitsLoss()

# model_optimizer = torch.optim.RMSprop(model.parameters(), lr=4e-3)
# model_optimizer = torch.optim.SGD(model.parameters(), lr=0.01)


# print(batch_size*len(train_data))
# print(batch_size*len(test_data))

print("[Train Top 1, Train Top 3, Train Top 5], [Test Top 1, Test Top 3, Test Top 5]")



def objective(trial):
	print(trial.number)

	optuna_accuracy = 0
	
	batch_size = trial.suggest_int("batch_size", 32, 256, step = 32)
	
	train_data, test_data = getData(batch_size)
	
	num_heads = trial.suggest_int("num_heads", 1, 50, step = 3)
	head_size = trial.suggest_int("head_size", 10, 100, step = 10)
	embeds_size = num_heads*head_size # d_model (model dimensions)

	n_enc_blocks = trial.suggest_int("n_enc_blocks", 1, 20)

	model = transformer(embeds_size, num_heads,n_enc_blocks).to(device)
	model_loss = nn.CrossEntropyLoss()
	optimizer_name = trial.suggest_categorical("optimizer", ["Adam", "RMSprop", "SGD"])
	lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
	model_optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)
	
	#embeds_size = 40 # d_model (model dimensions)
	


	#head_size = embeds_size // num_heads
	
	#model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
	for epoch in range(epochs):
		losses = 0
		# i = 0
		for (inputs, targets, mask) in train_data:
			# print(inputs.shape,targets)
			inputs = inputs.to(device)
			targets = targets.to(device)
			output = model(inputs, mask)
			loss = model_loss(output, targets)
			model_optimizer.zero_grad()
			loss.backward()
			model_optimizer.step()
			losses += loss.item()
			# print(f'{i*100/len(train_data)}')
			# i+=1
		print(f'[{epoch}][Train]', losses/len(train_data))
		model.eval()
		test_loss = 0
		train_passed = 0
		train_passed_top_3 = 0
		train_passed_top_5 = 0

		test_passed = 0
		test_passed_top_3 = 0
		test_passed_top_5 = 0

		for (inputs, targets, mask) in train_data:
			with torch.no_grad():
				inputs = inputs.to(device)
				targets = targets.to(device)
				outputs = model(inputs, mask)
				# print("Len Outputs", len(outputs))
				# print("Outputs", outputs)
				# print(outputs.shape)
				# print("Output Argmax", outputs.argmax(dim=1))
				# print("Targets", targets)
				# print("Targets", targets.argmax(dim=1))
				# ans = outputs.argmax(dim=1)
				# print (ans.shape)
				train_passed += torch.sum(outputs.argmax(dim=1) == targets.argmax(dim=1))
				metric = MulticlassAccuracy(average='micro',k=3,device=device)
				metric.update(outputs, targets.argmax(dim=1))
				train_passed_top_3 += metric.compute()

				metric = MulticlassAccuracy(average='micro',k=5,device=device)
				metric.update(outputs, targets.argmax(dim=1))
				train_passed_top_5 += metric.compute()
				# print (top_k_acc)

		for (inputs, targets, mask) in test_data:
			with torch.no_grad():
				inputs = inputs.to(device)
				targets = targets.to(device)
				outputs = model(inputs, mask)
				# print("Len Outputs", len(outputs))
				# print("Outputs", outputs)
				# print(outputs.shape)
				# print("Output Argmax", outputs.argmax(dim=1))
				# print("Targets", targets)
				# print("Targets", targets.argmax(dim=1))
				# ans = outputs.argmax(dim=1)
				# print (ans.shape)
				test_passed += torch.sum(outputs.argmax(dim=1) == targets.argmax(dim=1))
				metric = MulticlassAccuracy(average='micro',k=3,device=device)
				metric.update(outputs, targets.argmax(dim=1))
				test_passed_top_3 += metric.compute()

				metric = MulticlassAccuracy(average='micro',k=5,device=device)
				metric.update(outputs, targets.argmax(dim=1))
				test_passed_top_5 += metric.compute()
				# print (top_k_acc)
		model.train()
		# print(batch_size*len(train_data))
		# print(batch_size*len(test_data))
		print(f'[{epoch}][{train_passed / (batch_size*len(train_data))}, {train_passed_top_3 / len(train_data)}, {train_passed_top_5 / len(train_data)}] [{test_passed / (batch_size*len(test_data))}, {test_passed_top_3 / len(test_data)}, {test_passed_top_5 / len(test_data)}]')
		
		optuna_accuracy = test_passed_top_5 / len(test_data)
		# print(f'[{epoch}][Train Top 3]', ', accuracy', train_passed_top_3 / len(train_data))
		# print(f'[{epoch}][Train Top 5]', ', accuracy', train_passed_top_5 / len(train_data))
		# print(f'[{epoch}][Test Top 1]', ', accuracy', test_passed / (batch_size*len(test_data)))
		# print(f'[{epoch}][Test Top 3]', ', accuracy', test_passed_top_3 / len(test_data))
		# print(f'[{epoch}][Test Top 5]', ', accuracy', test_passed_top_5 / len(test_data))

		
	return optuna_accuracy


#torch.save(model.state_dict(), model_path)

#print(test_indices)

# i = random.randrange(0, data_to_use)
# statement_test = [i]

# test_single_sampler = SubsetRandomSampler(statement_test)
# dataloader_test = DataLoader(complete_dataset, 1, sampler=test_single_sampler)
# print(data[i][0])
# print(data[i][1])

# model.eval()
# for (inputs, targets, mask) in dataloader_test:
  # with torch.no_grad():
    # inputs = inputs.to(device)
    # targets = targets.to(device)
    # outputs = model(inputs, mask)
    # # print(outputs)
    # print(outputs.argmax())
    # print(torch.topk(outputs,5).indices)
    # # if
    # print(targets.argmax())
    # # if outputs.argmax() == targets.argmax():
    # #   p
    # #   passed += 1

# run pre trained model

#wDATA_DIR = '/content/drive/MyDrive/Colab Notebooks/mmpy/'
#os.chdir(DATA_DIR)
#model_path = 'model_classification.pth'

# model
#model = transformer()
#model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
# model = torch.load(model_path, map_location=torch.device('cpu'))
# model.load_state_dict(torch.load(model_path))
# model = model['model']
# model.to(device)

if __name__ == "__main__":
	study = optuna.create_study(direction="maximize")
	study.optimize(objective, n_trials=25)

	pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
	complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

	print("Study statistics: ")
	print("  Number of finished trials: ", len(study.trials))
	print("  Number of pruned trials: ", len(pruned_trials))
	print("  Number of complete trials: ", len(complete_trials))

	print("Best trial:")
	trial = study.best_trial

	print("  Value: ", trial.value)

	print("  Params: ")
	for key, value in trial.params.items():
		print("    {}: {}".format(key, value))

